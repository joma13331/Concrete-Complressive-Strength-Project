import os
import numpy as np
import logging
from sklearn.linear_model import Ridge, Lasso
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV
from sklearn.metrics import r2_score


class CCSModelFinderTrain:
    """
    :Class Name: CCSModelFinderTrain
    :Description: This class will be used to train different models and select the best one
                  amongst them.

    Written By: Jobin Mathew
    Interning at iNeuron Intelligence
    Version: 1.0
    """

    def __init__(self):

        if not os.path.isdir("CCSLogFiles/training/"):
            os.mkdir("CCSLogFiles/training/")

        self.log_path = "CCSLogFiles/training/CCSModelFinderTrain.txt"

        self.ccs_model_finder_logging = logging.getLogger("ccs_model_finder_log")
        self.ccs_model_finder_logging.setLevel(logging.INFO)
        ccs_model_finder_handler = logging.FileHandler(self.log_path)
        formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s',
                                      datefmt='%m/%d/%Y %I:%M:%S %p')
        ccs_model_finder_handler.setFormatter(formatter)
        self.ccs_model_finder_logging.addHandler(ccs_model_finder_handler)

        self.operation = 'TRAINING'

        self.rfr = RandomForestRegressor(n_jobs=-1, verbose=0)
        self.xgb = XGBRegressor()
        self.ridge = Ridge()
        self.lasso = Lasso()
        self.svr = SVR()
        self.kfold = KFold(shuffle=True, random_state=42)

    def ccs_adj_r2(self, estimator, x, y_true):
        """
        :Method Name: ccs_adj_r2
        :Description: This method will be used by GridSearchCV to score the different models generated by adjusted r2
                      value.

        :param estimator: The sklearn model which will be fitted using GridSearchCV
        :param x: Input training data
        :param y_true: Output training labels
        :return: adjusted R2 score
        """
        n, p = x.shape
        # print(n, p)
        pred = estimator.predict(x)
        if n-p-1 < 1:
            return r2_score(y_true, pred)
        else:
            return 1 - ((1 - r2_score(y_true, pred)) * (n - 1)) / (n - p - 1)

    def ccs_best_ridge_regressor(self, train_x, train_y):
        """
        :Method Name: ccs_get_best_ridge_regressor
        :Description: This method trains and returns the best model amongst many trained ridge regressor.

        :param train_x: Input training Data
        :param train_y: Input training labels
        :return: The best ridge regressor model
        :On failure: Exception
        """
        try:

            param_grid = {
                'alpha': np.random.uniform(0, 10, 50),
                'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
            }
            message = f"{self.operation}: Using GridSearchCV to obtain the optimum parameters({param_grid.keys()})" \
                      f"  of Ridge Regressor"
            self.ccs_model_finder_logging.info(message)

            # GridSearchCV is used as there are only a few combination of parameters.
            grid = GridSearchCV(estimator=self.ridge, param_grid=param_grid,
                                cv=self.kfold, n_jobs=-1,
                                scoring={'n-mse': 'neg_mean_squared_error',
                                         'adjusted-R2': self.ccs_adj_r2},
                                refit='adjusted-R2', verbose=0)

            grid.fit(train_x, train_y)

            alpha = grid.best_params_['alpha']
            solver = grid.best_params_['solver']
            score = grid.best_score_

            message = f"{self.operation}: The optimum parameters of Ridge Regressor are alpha={alpha}, " \
                      f"solver={solver} with the adjusted R2 score of {score}"
            
            self.ccs_model_finder_logging.info(message)

            self.ridge = Ridge(alpha=alpha, solver=solver)
            self.ridge.fit(train_x, train_y)

            message = f"{self.operation}: Best Ridge Regressor trained"
            self.ccs_model_finder_logging.info(message)

            return self.ridge

        except Exception as e:
            message = f"{self.operation}: There was a problem while fitting Ridge Regressor: {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e
    
    def ccs_best_lasso_regressor(self, train_x, train_y):
        """
        :Method Name: ccs_get_best_lasso_regressor
        :Description: This method trains and returns the best model amongst many trained lasso regressor.

        :param train_x: Input training Data
        :param train_y: Input training labels
        :return: The best lasso regressor model
        :On failure: Exception
        """
        try:
            param_grid = {
                'alpha': np.random.uniform(0, 10, 50),
                'selection': ['cyclic', 'random']
            }
            message = f"{self.operation}: Using GridSearchCV to obtain the optimum parameters({param_grid.keys()})" \
                      f"  of Lasso Regressor"
            self.ccs_model_finder_logging.info(message)

            # GridSearchCV is used as there are only a few combination of parameters.
            grid = GridSearchCV(estimator=self.lasso, param_grid=param_grid,
                                cv=self.kfold, n_jobs=-1,
                                scoring={'n-mse': 'neg_mean_squared_error',
                                         'adjusted-R2': self.ccs_adj_r2},
                                refit='adjusted-R2', verbose=0)

            grid.fit(train_x, train_y)

            alpha = grid.best_params_['alpha']
            selection = grid.best_params_['selection']
            score = grid.best_score_

            message = f"{self.operation}: The optimum parameters of Lasso Regressor are alpha={alpha}," \
                      f" selection={selection} with the adjusted R2 score of {score}"
            self.ccs_model_finder_logging.info(message)

            self.lasso = Lasso(alpha=alpha, selection=selection)
            self.lasso.fit(train_x, train_y)

            message = f"{self.operation}: Best Lasso Regressor trained"
            self.ccs_model_finder_logging.info(message)
            
            return self.lasso

        except Exception as e:
            message = f"{self.operation}: There was a problem while fitting Lasso Regressor: {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e
        
    def ccs_best_svr(self, train_x, train_y):
        """
        :Method Name: ccs_get_best_svr
        :Description: This method trains and returns the best model amongst many trained SVR.

        :param train_x: Input training Data
        :param train_y: Input training labels
        :return: The best SVR model
        :On failure: Exception
        """

        try:
            param_grid = {
                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                'gamma': ['scale', 'auto'],
                'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10],
                'degree': [2, 3, 4, 5],
                'epsilon': [0.01, 0.03, 0.1, 0.3, 1, 3, 10]
            }

            message = f"{self.operation}: Using GridSearchCV to obtain the optimum parameters({param_grid.keys()})" \
                      f" of SVR"
            self.ccs_model_finder_logging.info(message)

            # GridSearchCV is used as there are only a few combination of parameters.
            grid = GridSearchCV(estimator=self.svr, param_grid=param_grid,
                                cv=self.kfold, n_jobs=-1,
                                scoring={'n-mse': 'neg_mean_squared_error',
                                         'adjusted-R2': self.ccs_adj_r2},
                                refit='adjusted-R2', verbose=0)

            grid.fit(train_x, train_y)

            kernel = grid.best_params_['kernel']
            gamma = grid.best_params_['gamma']
            c = grid.best_params_['C']
            degree = grid.best_params_['degree']
            epsilon = grid.best_params_['epsilon']
            score = grid.best_score_

            message = f"{self.operation}: The optimum parameters of SVR are kernel={kernel}, gamma={gamma}, C={c}," \
                      f" degree ={degree}, epsilon={epsilon} with the adjusted R2 score of {score}"
            self.ccs_model_finder_logging.info(message)

            self.svr = SVR(kernel=kernel, gamma=gamma, C=c, degree=degree, epsilon=epsilon)
            self.svr.fit(train_x, train_y)

            message = f"{self.operation}: Best SVR trained"
            self.ccs_model_finder_logging.info(message)

            return self.svr

        except Exception as e:
            message = f"{self.operation}: There was a problem while fitting SVR: {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e
    
    def ccs_best_random_forest(self, train_x, train_y):
        """
        :Method Name: ccs_best_random_forest
        :Description: This method trains and returns the best model amongst many trained random forest regressor.

        :param train_x: Input training Data
        :param train_y: Input training labels
        :return: The best random forest regressor model
        :On failure: Exception"""

        try:
            param_grid = {
                "n_estimators": [30, 50, 100, 130, 150],
                'criterion': ['mse', 'mae'],
                'min_samples_split': [2, 3, 4, 5, 6, 7, 8],
                'max_features': ['auto', 'sqrt', 'log2'],
                'ccp_alpha': np.arange(0, 0.012, 0.001),

            }

            message = f"{self.operation}: Using GridSearchCV to obtain the optimum parameters({param_grid.keys()})" \
                      f" of random forest regressor "
            self.ccs_model_finder_logging.info(message)

            # RandomSearchCV is used as there are a large number combination of parameters.
            grid = RandomizedSearchCV(estimator=self.rfr, param_distributions=param_grid, n_iter=500,
                                      cv=self.kfold, n_jobs=-1,
                                      scoring={'n-mse': 'neg_mean_squared_error',
                                               'adjusted-R2': self.ccs_adj_r2},
                                      refit='adjusted-R2', verbose=0)

            grid.fit(train_x, train_y)

            n_estimators = grid.best_params_['n_estimators']
            criterion = grid.best_params_['criterion']
            min_samples_split = grid.best_params_['min_samples_split']
            max_features = grid.best_params_['max_features']
            ccp_alpha = grid.best_params_['ccp_alpha']
            score = grid.best_score_

            message = f"{self.operation}: The optimum parameters of random forrest regressor are " \
                      f"n_estimators={n_estimators}, criterion={criterion}, min_samples_split={min_samples_split}," \
                      f" max_features ={max_features}, ccp_alpha={ccp_alpha} with the adjusted R2 score of {score}"
            self.ccs_model_finder_logging.info(message)

            self.rfr = RandomForestRegressor(n_jobs=-1, verbose=0,
                                             n_estimators=n_estimators, criterion=criterion,
                                             min_samples_split=min_samples_split,
                                             max_features=max_features, ccp_alpha=ccp_alpha
                                             )
            self.rfr.fit(train_x, train_y)

            message = f"{self.operation}: Best random forest regressor trained"
            self.ccs_model_finder_logging.info(message)
            
            return self.rfr

        except Exception as e:
            message = f"{self.operation}: There was a problem while fitting Random Forest Regressor: {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e

    def ccs_best_xgb_regressor(self, train_x, train_y):
        """
        :Method Name: ccs_best_xgb_regressor
        :Description: This method trains and returns the best model amongst many trained xgb regressors.

        :param train_x: Input training Data
        :param train_y: Input training labels
        :return: The best xgb regressor model
        :On failure: Exception
        """

        try:
            param_grid = {
                'learning_rate': [0.01, 0.03, 0.1, 0.3, 1],
                'colsample_bytree': [.1, .2, .3, .4, .5, .6, .7, .8, .9],
                'max_depth': [3, 5, 10, 15, 20],
                'n_estimators': [30, 100, 300, 1000, 3000],
                "verbosity": [0]
            }

            message = f"{self.operation}: Using GridSearchCV to obtain the optimum parameters({param_grid.keys()}) " \
                      f"of xgb regressor"
            self.ccs_model_finder_logging.info(message)

            # RandomSearchCV is used as there are a large number combination of parameters.
            grid = RandomizedSearchCV(estimator=self.xgb, param_distributions=param_grid, n_iter=250,
                                      cv=self.kfold, n_jobs=-1,
                                      scoring={'n-mse': 'neg_mean_squared_error',
                                               'adjusted-R2': self.ccs_adj_r2},
                                      refit='adjusted-R2', verbose=0)
            grid.fit(train_x, train_y)

            learning_rate = grid.best_params_['learning_rate']
            colsample_bytree = grid.best_params_['colsample_bytree']
            max_depth = grid.best_params_['max_depth']
            n_estimators = grid.best_params_['n_estimators']
            score = grid.best_score_

            message = f"{self.operation}: The optimum parameters of xgb-regressor are learning_rate={learning_rate}, " \
                      f"max_depth={max_depth}, colsample_bytree={colsample_bytree}, n_estimators ={n_estimators} " \
                      f"with the adjusted R2 score of {score}"
            self.ccs_model_finder_logging.info(message)

            self.xgb = XGBRegressor(n_jobs=-1, verbose=0, learning_rate=learning_rate,
                                    colsample_bytree=colsample_bytree,
                                    max_depth=max_depth, n_estimators=n_estimators)

            self.xgb.fit(train_x, train_y)
            
            message = f"{self.operation}: Best xgb regressor trained"
            self.ccs_model_finder_logging.info(message)
            return self.xgb

        except Exception as e:
            message = f"{self.operation}: There was a problem while fitting Random Forest Regressor: {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e
    
    def ccs_best_model_from_adj_r2(self, r2_adj):
        """
        :Method Name: ccs_best_model_from_adj_r2
        :Description: This method takes in a dictionary with model name as keys and adjusted r2 score as values,
                      it then returns the best model based on highest adjusted r2 score.

        :param r2_adj: The dictionary of all adjusted r2 scores
        :return: The best sklearn model for the given dataset
        :On Failure: Exception
        """
        try:
            keys = list(r2_adj.keys())
            values = list(r2_adj.values())
            ind = values.index(max(values))
            
            if keys[ind] == "ridge":
                message = f"{self.operation}: The best model is ridge regressor"
                self.ccs_model_finder_logging.info(message)
                return keys[ind], self.ridge
            
            elif keys[ind] == "lasso":
                message = f"{self.operation}: The best model is lasso regressor"
                self.ccs_model_finder_logging.info(message)
                return keys[ind], self.lasso
            
            elif keys[ind] == "svr":
                message = f"{self.operation}: The best model is svr"
                self.ccs_model_finder_logging.info(message)
                return keys[ind], self.svr
            
            elif keys[ind] == "rfr":
                message = f"{self.operation}: The best model is random forest regressor"
                self.ccs_model_finder_logging.info(message)
                return keys[ind], self.rfr
            
            else:
                message = f"{self.operation}: The best model is xgb regressor"
                self.ccs_model_finder_logging.info(message)
                return keys[ind], self.xgb

        except Exception as e:
            message = f"{self.operation}: There was a problem while obtaining best model from adjusted r2 " \
                      f"dictionary: {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e

    def ccs_best_model(self, train_x, train_y, test_x, test_y):
        """
        :Method Name: ccs_best_model
        :Description: This method is used to select the best model from all the best model from all categories.

        :param train_x: the training features
        :param train_y: the training labels
        :param test_x: the test features
        :param test_y: the test labels
        :return: The best sklearn model for the given dataset
        :On Failure: Exception
        """

        try:
            message = f"{self.operation}: Search for best model started"
            self.ccs_model_finder_logging.info(message)

            r2_adj = {}

            message = f"{self.operation}: Search for best ridge model started"
            self.ccs_model_finder_logging.info(message)

            self.ridge = self.ccs_best_ridge_regressor(train_x, train_y)
            r2_adj["ridge"] = self.ccs_adj_r2(self.ridge, test_x, test_y)

            message = f"{self.operation}: Search for best ridge model ended"
            self.ccs_model_finder_logging.info(message)

            message = f"{self.operation}: Search for best lasso model started"
            self.ccs_model_finder_logging.info(message)

            self.lasso = self.ccs_best_lasso_regressor(train_x, train_y)
            r2_adj["lasso"] = self.ccs_adj_r2(self.lasso, test_x, test_y)

            message = f"{self.operation}: Search for best lasso model ended"
            self.ccs_model_finder_logging.info(message)

            message = f"{self.operation}: Search for best svr model started"
            self.ccs_model_finder_logging.info(message)

            self.svr = self.ccs_best_svr(train_x, train_y)
            r2_adj["svr"] = self.ccs_adj_r2(self.svr, test_x, test_y)

            message = f"{self.operation}: Search for best svr model ended"
            self.ccs_model_finder_logging.info(message)

            message = f"{self.operation}: Search for best random forest regressor model started"
            self.ccs_model_finder_logging.info(message)

            self.rfr = self.ccs_best_random_forest(train_x, train_y)
            r2_adj["rfr"] = self.ccs_adj_r2(self.rfr, test_x, test_y)

            message = f"{self.operation}: Search for best random forest regressor model ended"
            self.ccs_model_finder_logging.info(message)

            message = f"{self.operation}: Search for best xgb regressor model started"
            self.ccs_model_finder_logging.info(message)

            self.xgb = self.ccs_best_xgb_regressor(train_x, train_y)
            r2_adj["xgb"] = self.ccs_adj_r2(self.xgb, test_x, test_y)

            message = f"{self.operation}: Search for best xgb regressor model ended"
            self.ccs_model_finder_logging.info(message)

            return self.ccs_best_model_from_adj_r2(r2_adj)

        except Exception as e:
            message = f"{self.operation}: There was a problem while obtaining best model : {str(e)}"
            self.ccs_model_finder_logging.error(message)
            raise e
    